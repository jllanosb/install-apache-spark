# INSTALACI√ìN DE APACHE SPARK EN UBUNTU
Apache Spark es un framework de procesamiento de datos de c√≥digo abierto dise√±ado para trabajar con grandes vol√∫menes de informaci√≥n de manera r√°pida, distribuida y tolerante a fallos.
Es una evoluci√≥n de Hadoop MapReduce, ofreciendo un rendimiento mucho mayor gracias a su uso intensivo de procesamiento en memoria (in-memory computing).

Se utiliza en:
- Big Data Analytics
- Machine Learning
- Procesamiento de flujos en tiempo real
- SQL distribuido
- Integraci√≥n con bases de datos y sistemas de almacenamiento distribuidos (HDFS, S3, Cassandra, etc.)

## üß© Componentes principales de Apache Spark
1. ‚≠ê Spark Core

Es el coraz√≥n del sistema.
Proporciona:
- Gesti√≥n de tareas distribuidas
- Gesti√≥n de memoria
- Recuperaci√≥n ante fallos
- API para RDD (Resilient Distributed Datasets)

2. üóÑÔ∏è Spark SQL

Permite ejecutar:
- Consultas SQL
- Operaciones sobre DataFrames y Datasets
- Integraci√≥n con cat√°logos de datos (Hive, etc.)

3. üìä Spark Streaming / Structured Streaming

Para procesamiento de datos en tiempo real.
Structured Streaming es la versi√≥n moderna y m√°s robusta.

4. üß† MLlib

Biblioteca de Machine Learning distribuido:
- Clasificaci√≥n
- Regresi√≥n
- Clustering
- Recomendaci√≥n

5. üåê GraphX

Para trabajar con grafos de forma distribuida (PageRank, an√°lisis de redes, etc.)

6. üöÄ Cluster Managers

Spark puede ejecutarse sobre varios sistemas de gesti√≥n de cl√∫ster:
- **Standalone** (incluido con Spark)
- **YARN** (Hadoop)
- **Mesos**
- **Kubernetes**

# üñ•Ô∏è Requisitos m√≠nimos para instalar Apache Spark
‚úîÔ∏è Requisitos de software
- **Java** 8, 11 o 17 (seg√∫n versi√≥n de Spark)
- **Python** (si planeas usar PySpark): 3.7 o superior
- **Scala** (opcional): usualmente 2.12 o 2.13
- **Hadoop** (opcional): solo necesario si usar√°s HDFS o YARN

Spark puede correr **sin Hadoop** en modo Standalone.

# üß∞ Requisitos m√≠nimos de hardware (modo local)

Para pruebas y aprendizaje en tu PC:

üî∏ M√≠nimo funcional:
- 1 CPU
- 4 GB RAM
- 10 GB de espacio en disco

üî∏ Recomendado:
- 4 CPU
- 8‚Äì16 GB de RAM
- SSD, NVME si posible

## üè≠ Requisitos t√≠picos para un cl√∫ster Spark (producci√≥n)

Depende de la carga, pero com√∫nmente:
- **Nodos de trabajo (Workers)**: 4‚Äì16 CPU cada uno
- **RAM por nodo**: entre 32‚Äì128 GB
- **Red**: 10 Gbps
- **Almacenamiento**: HDFS o S3

Nota. Tener instalado Ubuntu 24.04 / usuario hadoop / Java 11 con [Hadoop](https://github.com/jllanosb/install-apache-hadoop) + [Hive Metastore PostgreSQL](https://github.com/jllanosb/install-apache-hive)

# 1. Descargar Apache Spark

Recomendaci√≥n estable: Spark 3.5.7 (con Hadoop 3.4.2 prebuilt, compatible con 3.4.x runtime sin conflicto)
```bash
cd /tmp
wget https://dlcdn.apache.org/spark/spark-4.0.1/spark-4.0.1-bin-hadoop3.tgz
sudo mkdir -p /opt/spark
sudo tar -xzf spark-4.0.1-bin-hadoop3.tgz -C /opt/spark --strip-components=1
sudo chown -R hadoop:hadoop /opt/spark
```

# 2. Variables de entorno Spark
```bash
sudo -u hadoop nano ~/.bashrc
```
agregar:
```bash
# Apache Spark
export SPARK_HOME=/opt/spark
export PATH=$PATH:$SPARK_HOME/bin:$SPARK_HOME/sbin
export PYSPARK_PYTHON=/usr/bin/python3
export HADOOP_CONF_DIR=/opt/hadoop/etc/hadoop
export YARN_CONF_DIR=/opt/hadoop/etc/hadoop
```
```bash
source ~/.bashrc
```
# 3. Integraci√≥n Spark + Hadoop

Spark debe apuntar al mismo config de Hadoop para que YARN lo gestione.
```bash
sudo -u hadoop cp $SPARK_HOME/conf/spark-env.sh.template $SPARK_HOME/conf/spark-env.sh
sudo -u hadoop nano $SPARK_HOME/conf/spark-env.sh
```
agregar:
```bash
# Java For Spark
export JAVA_HOME=/usr/lib/jvm/java-11-openjdk-amd64
export HADOOP_CONF_DIR=/opt/hadoop/etc/hadoop
export YARN_CONF_DIR=/opt/hadoop/etc/hadoop
```
# 4. Integrar Hive Metastore con Spark SQL
```bash
sudo -u hadoop cp /opt/hive/conf/hive-site.xml /opt/spark/conf/
```
Spark ya puede leer Metastore PostgreSQL.
# 5. Prueba avanzada Spark on YARN

Levantar YARN y DFS (si no est√° arriba)
```bash
start-dfs.sh
start-yarn.sh
```
Verificar
```bash
hdfs dfs -ls /user/hadoop/
hdfs dfs -chmod -R 755 /user/hadoop
hdfs dfs -rm -r /user/hadoop/.sparkStaging
```
Revisar la configuracion yarn-env.sh
```bash 
sudo nano $HADOOP_HOME/etc/hadoop/yarn-env.sh
```

Verificar y/o agregar 
```bash
export JAVA_HOME=/usr/lib/jvm/java-17-openjdk-amd64
```

Verificar jars
```bash
ls /opt/spark/examples/jars/
```
Identifica que version de jar se tiene _2.12-3.5.7.jar or _2.13-4.0.1.jar
```
spark-examples_2.13-4.0.1.jar
```
# 6. Probar Funcionamiento de Spark
Ejecutar job spark ejemplo sobre YARN: segun la version jar (2.12-3.5.7)
```bash
spark-submit --master yarn --deploy-mode client \
  --class org.apache.spark.examples.SparkPi \
  $SPARK_HOME/examples/jars/spark-examples_2.12-3.5.7.jar 1000
```

Ejecuta el siguiente codigo si falla el codigo anterior al configurar el tama√±o de memoria
```bash
spark-submit \
  --master yarn \
  --deploy-mode client \
  --conf spark.yarn.jars=local:///opt/spark/jars/* \
  --class org.apache.spark.examples.SparkPi \
  $SPARK_HOME/examples/jars/spark-examples_2.12-3.5.7.jar 1000
```

Ejecutar job spark ejemplo sobre YARN: segun la version jar (2.12-3.5.7)
```bash
spark-submit --master yarn --deploy-mode client \
  --class org.apache.spark.examples.SparkPi \
  $SPARK_HOME/examples/jars/spark-examples_2.13-4.0.1.jar 1000
```
Ejecuta el siguiente codigo si falla el codigo anterior al configurar el tama√±o de memoria
```bash
spark-submit \
  --master yarn \
  --deploy-mode client \
  --conf spark.yarn.jars=local:///opt/spark/jars/* \
  --class org.apache.spark.examples.SparkPi \
  $SPARK_HOME/examples/jars/spark-examples_2.13-4.0.1.jar 1000
```

Si corre sin error ‚Üí Spark on Yarn OK + integraci√≥n Hadoop OK.
Hasta aqu√≠ tenemos:
Componente	Estado
Hadoop /opt/hadoop	‚úÖ
Hive Metastore PostgreSQL	‚úÖ
Spark on YARN /opt/spark	‚úÖ integrado