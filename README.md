# INSTALACIÃ“N DE APACHE SPARK EN UBUNTU
Apache Spark es un framework de procesamiento de datos de cÃ³digo abierto diseÃ±ado para trabajar con grandes volÃºmenes de informaciÃ³n de manera rÃ¡pida, distribuida y tolerante a fallos.
Es una evoluciÃ³n de Hadoop MapReduce, ofreciendo un rendimiento mucho mayor gracias a su uso intensivo de procesamiento en memoria (in-memory computing).

Se utiliza en:
- Big Data Analytics
- Machine Learning
- Procesamiento de flujos en tiempo real
- SQL distribuido
- IntegraciÃ³n con bases de datos y sistemas de almacenamiento distribuidos (HDFS, S3, Cassandra, etc.)

## ğŸ§© Componentes principales de Apache Spark
1. â­ Spark Core

Es el corazÃ³n del sistema.
Proporciona:
- GestiÃ³n de tareas distribuidas
- GestiÃ³n de memoria
- RecuperaciÃ³n ante fallos
- API para RDD (Resilient Distributed Datasets)

2. ğŸ—„ï¸ Spark SQL

Permite ejecutar:
- Consultas SQL
- Operaciones sobre DataFrames y Datasets
- IntegraciÃ³n con catÃ¡logos de datos (Hive, etc.)

3. ğŸ“Š Spark Streaming / Structured Streaming

Para procesamiento de datos en tiempo real.
Structured Streaming es la versiÃ³n moderna y mÃ¡s robusta.

4. ğŸ§  MLlib

Biblioteca de Machine Learning distribuido:
- ClasificaciÃ³n
- RegresiÃ³n
- Clustering
- RecomendaciÃ³n

5. ğŸŒ GraphX

Para trabajar con grafos de forma distribuida (PageRank, anÃ¡lisis de redes, etc.)

6. ğŸš€ Cluster Managers

Spark puede ejecutarse sobre varios sistemas de gestiÃ³n de clÃºster:
- **Standalone** (incluido con Spark)
- **YARN** (Hadoop)
- **Mesos**
- **Kubernetes**

# ğŸ–¥ï¸ Requisitos mÃ­nimos para instalar Apache Spark
âœ”ï¸ Requisitos de software
- **Java** 8, 11 o 17 (segÃºn versiÃ³n de Spark)
- **Python** (si planeas usar PySpark): 3.7 o superior
- **Scala** (opcional): usualmente 2.12 o 2.13
- **Hadoop** (opcional): solo necesario si usarÃ¡s HDFS o YARN

Spark puede correr **sin Hadoop** en modo Standalone.

# ğŸ§° Requisitos mÃ­nimos de hardware (modo local)

Para pruebas y aprendizaje en tu PC:

ğŸ”¸ MÃ­nimo funcional:
- 1 CPU
- 4 GB RAM
- 10 GB de espacio en disco

ğŸ”¸ Recomendado:
- 4 CPU
- 8â€“16 GB de RAM
- SSD, NVME si posible

## ğŸ­ Requisitos tÃ­picos para un clÃºster Spark (producciÃ³n)

Depende de la carga, pero comÃºnmente:
- **Nodos de trabajo (Workers)**: 4â€“16 CPU cada uno
- **RAM por nodo**: entre 32â€“128 GB
- **Red**: 10 Gbps
- **Almacenamiento**: HDFS o S3

Nota. Tener instalado Ubuntu 24.04 / usuario hadoop / Java 11 con [Hadoop](https://github.com/jllanosb/install-apache-hadoop) + [Hive Metastore PostgreSQL](https://github.com/jllanosb/install-apache-hive)

# 1. Descargar Apache Spark

RecomendaciÃ³n estable: Spark 3.5.7 (con Hadoop 3.4.2 prebuilt, compatible con 3.4.x runtime sin conflicto)
```bash
cd /tmp
wget https://dlcdn.apache.org/spark/spark-4.0.1/spark-4.0.1-bin-hadoop3.tgz
sudo mkdir -p /opt/spark
sudo tar -xzf spark-4.0.1-bin-hadoop3.tgz -C /opt/spark --strip-components=1
sudo chown -R hadoop:hadoop /opt/spark
```